{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4ChemS/CHE-1147/blob/main/tutorials/tutorial_02_linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNGZ5hoEJq4p"
      },
      "source": [
        "# Why Linear Algebra Matters for ML\n",
        "\n",
        "Machine learning models are built on **linear algebra**.  \n",
        "Almost everything we do with data involves vectors and matrices:\n",
        "\n",
        "- **Data as vectors:**  \n",
        "  Each sample = a vector of features  \n",
        "  (e.g., a molecule described by [MW, boiling point, density]).\n",
        "\n",
        "- **Datasets as matrices:**  \n",
        "  Rows = samples, Columns = features.  \n",
        "  Example: 100 molecules × 10 properties → a 100×10 matrix.\n",
        "\n",
        "- **Model parameters as matrices:**  \n",
        "  Linear regression, neural networks, PCA, and embeddings all rely on matrix multiplications.\n",
        "\n",
        "- **Optimization:**  \n",
        "  Norms and dot products define distances and similarities.  \n",
        "  Eigenvectors and SVD power dimensionality reduction.\n",
        "\n",
        "👉 By reviewing a few essentials, you’ll be able to:  \n",
        "- Understand ML math notation quickly.  \n",
        "- Write correct and efficient NumPy code.  \n",
        "- Connect concepts (e.g., regression = solving linear systems, PCA = eigen decomposition).\n",
        "\n",
        "\n",
        "Further reading:\n",
        "- [Goodfellow's deep learning, Linear Algebra subchapter](https://www.deeplearningbook.org/contents/linear_algebra.html) Good math overview.\n",
        "- [Andrew White's Deep Learning for Molecules and Materials Book, Tensor and shapes](https://github.com/whitead/dmol-book/blob/main/math/tensors-and-shapes.ipynb) - Focused on notation and tensors for deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR2KnLWfJ22X"
      },
      "source": [
        "# 1. Vectors\n",
        "\n",
        "A **vector** is just an ordered list of numbers.  \n",
        "- In math: 1-d array of numbers\n",
        "- In ML, vectors represent **data points**.  \n",
        "A dataset = collection of vectors stacked into a matrix.\n",
        "\n",
        "Example: A data point for molecule described by three features:\n",
        "$$\n",
        "  \\text{CO₂} = [\\text{MW}, \\text{Boiling Point}, \\text{Density}]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use NumPy for vector operation for now. NumPy arrays are the fundamental way to store and operate on numerical data in Python for scientific computing and machine learning.\n",
        "\n",
        "- **Array structure:**  \n",
        "    A NumPy array (`np.ndarray`) is a grid of values, all of the same type, indexed by a tuple of nonnegative integers (its shape).\n",
        "\n",
        "- **1D array = vector:**  \n",
        "    Example:  \n",
        "    ```python\n",
        "    co2 = np.array([44.01, -78.5, 1.98])\n",
        "    ```\n",
        "    Here, `co2` is a 1D array (vector) with 3 elements.\n",
        "\n",
        "- **Key features:**  \n",
        "    - **Efficient storage:** Arrays use less memory and are much faster than Python lists for numerical operations.\n",
        "    - **Elementwise operations:** You can add, subtract, multiply, or divide arrays directly:\n",
        "        ```python\n",
        "        a = np.array([1, 2, 3])\n",
        "        b = np.array([4, 5, 6])\n",
        "        print(a + b)  # [5 7 9]\n",
        "        ```\n",
        "    - **Broadcasting:** NumPy automatically expands arrays of different shapes for compatible operations.\n",
        "    - **Slicing and indexing:** Access or modify parts of the array easily:\n",
        "        ```python\n",
        "        print(co2[0])    # 44.01\n",
        "        print(co2[1:])   # [-78.5, 1.98]\n",
        "        ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlsLMUJYJ7yt",
        "outputId": "2902e8f2-f998-4d75-d85d-ad42d7e0ffcd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example: feature vector for CO2\n",
        "co2 = np.array([44.01, -78.5, 1.98])   # MW, boiling point (°C), density (g/L)\n",
        "print(\"CO2 vector:\", co2)\n",
        "print(\"Shape:\", co2.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfSnTaMjJsIJ",
        "outputId": "a0d9bf1a-1884-498a-e86e-7244e7cf7a4a"
      },
      "outputs": [],
      "source": [
        "# Another molecule: H2O\n",
        "h2o = np.array([18.02, 100.0, 0.997])\n",
        "\n",
        "# Vector addition (elementwise)\n",
        "print(\"CO2 + H2O:\", co2 + h2o)\n",
        "\n",
        "# Scalar multiplication\n",
        "print(\"2 * CO2:\", 2 * co2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34K57vP6KAid"
      },
      "source": [
        "- **Addition** → combining feature contributions  \n",
        "- **Scalar multiplication** → rescaling all features (like changing units)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vector addition: add co2 and h2o vectors elementwise\n",
        "sum_vec = co2 + h2o\n",
        "print(\"Vector addition (CO2 + H2O):\", sum_vec)\n",
        "\n",
        "# Scalar multiplication: multiply co2 vector by 3\n",
        "scaled_vec = 3 * co2\n",
        "print(\"Scalar multiplication (3 * CO2):\", scaled_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1_XEWGxKICT"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Create a vector for methane (CH₄): `[16.04, -161.5, 0.717]`.  \n",
        "2. Add it to the CO₂ vector. What does the result mean?  \n",
        "3. Multiply the H₂O vector by 0.5. Interpret the result (half the feature values).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bsAbZhwKVR8"
      },
      "source": [
        "# 2. Dot Product & Norms\n",
        "\n",
        "These two concepts are the backbone of ML:\n",
        "\n",
        "- **Dot product:** measures alignment/similarity of two vectors.  \n",
        "- **Norm:** measures the length (magnitude) of a vector.\n",
        "\n",
        "They show up in:\n",
        "- Cosine similarity (used in embeddings, recommendation systems).  \n",
        "- Distances between data points.  \n",
        "- Optimization (gradient descent steps use vector norms).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5krUNOlKHlt",
        "outputId": "84e0a134-807d-45b8-809a-cf448cd2f8ac"
      },
      "outputs": [],
      "source": [
        "# Feature vectors for two molecules\n",
        "co2 = np.array([44.01, -78.5, 1.98])   # MW, BP, density\n",
        "h2o = np.array([18.02, 100.0, 0.997])\n",
        "\n",
        "# Dot product\n",
        "dot = np.dot(co2, h2o)\n",
        "print(\"Dot product:\", dot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsBK3lNqKd-W"
      },
      "source": [
        "The dot product is the sum of elementwise multiplications:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}\\cdot\\mathbf{b}=\\sum_{i=1}^{n} a_i\\,b_i\n",
        "$$\n",
        "\n",
        "\n",
        "- Large positive → vectors point in similar directions.  \n",
        "- Near zero → vectors are orthogonal (unrelated).  \n",
        "- Negative → vectors point in opposite directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ipywidgets if not already installed\n",
        "# %pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dot Product Interactive Visualization\n",
        "from ipywidgets import interact, FloatSlider, VBox, HBox\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_vectors(x1=-1.0, y1=1.0, x2=1.0, y2=1.0):\n",
        "    v1 = np.array([x1, y1])\n",
        "    v2 = np.array([x2, y2])\n",
        "    dot = np.dot(v1, v2)\n",
        "    \n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1, color='r', label='v1')\n",
        "    plt.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1, color='b', label='v2')\n",
        "    plt.xlim(-5, 5)\n",
        "    plt.ylim(-5, 5)\n",
        "    plt.axhline(0, color='gray', lw=0.5)\n",
        "    plt.axvline(0, color='gray', lw=0.5)\n",
        "    plt.gca().set_aspect('equal', 'box')\n",
        "    plt.legend()\n",
        "    plt.title(f\"Dot product: {dot:.2f}\")\n",
        "    plt.annotate(f\"dot = {dot:.2f}\", xy=(0.05, 0.9), xycoords='axes fraction', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "interact(\n",
        "    plot_vectors,\n",
        "    x1=FloatSlider(min=-4, max=4, step=0.1, value=-1.0, description='v1_x'),\n",
        "    y1=FloatSlider(min=-4, max=4, step=0.1, value=1.0, description='v1_y'),\n",
        "    x2=FloatSlider(min=-4, max=4, step=0.1, value=1.0, description='v2_x'),\n",
        "    y2=FloatSlider(min=-4, max=4, step=0.1, value=1.0, description='v2_y'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIQI2t84KsHY"
      },
      "source": [
        "- **L2 norm:** usual Euclidean distance (length of vector).  \n",
        "- **L1 norm:** sum of absolute values, often used for sparsity (e.g., Lasso regression).  \n",
        "\n",
        "In ML:\n",
        "- Norms measure the size of weights (regularization).  \n",
        "- Norms measure distances between feature vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9NcCJU5Kk1e",
        "outputId": "83ac87db-474c-4065-a862-7d2c671590b0"
      },
      "outputs": [],
      "source": [
        "# L2 norm (Euclidean length)\n",
        "norm_co2 = np.linalg.norm(co2)\n",
        "print(\"‖CO2‖ (L2 norm):\", norm_co2)\n",
        "\n",
        "# L1 norm (sum of absolute values)\n",
        "norm_co2_L1 = np.linalg.norm(co2, ord=1)\n",
        "print(\"‖CO2‖ (L1 norm):\", norm_co2_L1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cosine similarity measures the **angle** between two vectors, quantifying how similar their directions are regardless of their magnitude.  \n",
        "- Value ranges from -1 (opposite) to 1 (identical direction), with 0 meaning orthogonal (unrelated).\n",
        "- In ML, it's widely used to compare feature vectors, such as in word embeddings, recommendation systems, and clustering.  \n",
        "- Formula:  \n",
        "    $$\n",
        "    \\text{cosine\\_similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
        "    $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO4ZSIBEKpe1",
        "outputId": "72c25bc5-a632-4e74-b2a7-5dc782152d79"
      },
      "outputs": [],
      "source": [
        "# Cosine similarity = dot(a, b) / (‖a‖‖b‖)\n",
        "cos_sim = np.dot(co2, h2o) / (np.linalg.norm(co2) * np.linalg.norm(h2o))\n",
        "print(\"Cosine similarity (CO2 vs H2O):\", cos_sim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEyrwqdeK5cB"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Create a vector for methane: `[16.04, -161.5, 0.717]`.  \n",
        "2. Compute:\n",
        "   - Dot product with CO₂.  \n",
        "   - L2 norm.  \n",
        "   - Cosine similarity with H₂O.  \n",
        "3. Interpret the results: what does similarity mean in this feature space?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq1ggiUKLZ7D"
      },
      "source": [
        "# 3. Matrices\n",
        "\n",
        "A **matrix** is a 2D collection of numbers.  \n",
        "- In ML: rows = samples, columns = features.  \n",
        "- Example: dataset of molecules with 3 properties each → a matrix.\n",
        "\n",
        "Notation:\n",
        "- Vector = 1D array (shape `(n,)`)  \n",
        "- Matrix = 2D array (shape `(m, n)`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFdnaeROLcCm",
        "outputId": "7718c4c4-2b36-4867-9a6b-fbe202026ffd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Each row = [MW, Boiling Point, Density]\n",
        "data = np.array([\n",
        "    [44.01, -78.5, 1.98],   # CO2\n",
        "    [18.02, 100.0, 0.997],  # H2O\n",
        "    [16.04, -161.5, 0.717], # CH4\n",
        "    [32.00, -183.0, 1.429]  # O2\n",
        "])\n",
        "\n",
        "print(\"Dataset matrix:\\n\", data)\n",
        "print(\"Shape:\", data.shape)   # 4 samples × 3 features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0uCy41NLenV",
        "outputId": "fbdb9e2a-a65b-4607-9727-0244084a1446"
      },
      "outputs": [],
      "source": [
        "# Indexing & slicing\n",
        "\n",
        "# First row (CO2 vector)\n",
        "print(\"CO2:\", data[0])\n",
        "\n",
        "# First column (all MWs)\n",
        "print(\"Molecular weights:\", data[:, 0])\n",
        "\n",
        "# Submatrix: first 2 molecules, first 2 features\n",
        "print(\"Submatrix:\\n\", data[0:2, 0:2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_vq_3vsLiHY",
        "outputId": "ffe606fb-c934-4325-d5de-bf9fbd7b6e24"
      },
      "outputs": [],
      "source": [
        "# Transpose swaps rows and columns\n",
        "print(\"Original shape:\", data.shape)\n",
        "print(\"Transpose shape:\", data.T.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSNbKXmaLpsW"
      },
      "source": [
        "👉 Transpose is common in ML when we switch between:\n",
        "- Data in **row-major form** (samples = rows).  \n",
        "- Weight matrices that expect column vectors.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wQCoK7oLsTs"
      },
      "source": [
        "- **Row** = one molecule’s feature vector.  \n",
        "- **Column** = one property across all molecules.\n",
        "\n",
        "This dual view is why matrix algebra is so central:  \n",
        "- Operations can apply across rows (samples) or columns (features).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyAfsYOnLvvV"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Print the boiling points column (2nd column).  \n",
        "2. What is the average boiling point?  \n",
        "3. Extract the submatrix of `[MW, Density]` for H2O and CH4 only.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD7nyLphLzVD"
      },
      "source": [
        "# 4. Matrix Multiplication\n",
        "\n",
        "Matrix multiplication is everywhere in ML:\n",
        "- Linear regression → predictions = `X @ w`\n",
        "- Neural networks → layer outputs = `X @ W + b`\n",
        "- PCA → projecting data onto principal components\n",
        "\n",
        "**Rule:** To multiply `A (m × n)` and `B (n × p)`,  \n",
        "the inner dimensions must match (`n` = `n`), result is `m × p`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrSFvR6TL4a5",
        "outputId": "b09144dd-7d90-446d-d9fd-55ec5bc5368b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[1, 2],\n",
        "              [3, 4]])\n",
        "B = np.array([[5, 6],\n",
        "              [7, 8]])\n",
        "\n",
        "print(\"A:\\n\", A)\n",
        "print(\"B:\\n\", B)\n",
        "\n",
        "# Matrix multiplication\n",
        "C = A @ B   # same as np.dot(A, B)\n",
        "print(\"A @ B:\\n\", C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17PmGUpTL7wJ",
        "outputId": "e58e8883-3a99-46ab-912d-da0422aa1434"
      },
      "outputs": [],
      "source": [
        "# Dataset: rows = molecules, cols = features\n",
        "X = np.array([\n",
        "    [44.01, -78.5, 1.98],   # CO2\n",
        "    [18.02, 100.0, 0.997],  # H2O\n",
        "    [16.04, -161.5, 0.717], # CH4\n",
        "    [32.00, -183.0, 1.429]  # O2\n",
        "])\n",
        "\n",
        "# Weight vector (3 features → 1 output)\n",
        "w = np.array([0.01, 0.05, 2.0])\n",
        "\n",
        "# Prediction = X @ w\n",
        "y_pred = X @ w\n",
        "print(\"Predictions:\", y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkxISbBML-3-"
      },
      "source": [
        "Here:\n",
        "- `X` = data matrix (molecules × features)  \n",
        "- `w` = weight vector (features → output)  \n",
        "- `X @ w` = linear model predictions  \n",
        "\n",
        "👉 This is exactly what happens in **linear regression** and in every layer of a neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOBHpMfzMBuU",
        "outputId": "828eef39-4334-404d-873f-fdf140f767dc"
      },
      "outputs": [],
      "source": [
        "# Suppose we predict 2 different properties\n",
        "W = np.array([\n",
        "    [0.01, 0.02],   # feature 1 weights\n",
        "    [0.05, -0.03],  # feature 2 weights\n",
        "    [2.0, 1.0]      # feature 3 weights\n",
        "])\n",
        "\n",
        "Y_pred = X @ W\n",
        "print(\"Predictions (4 samples × 2 outputs):\\n\", Y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szgm1TSZMDTY"
      },
      "source": [
        "👉 Each column of `Y_pred` is a different predicted property.  \n",
        "This is how ML handle **multi-output prediction**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO79Ro-JMJkv"
      },
      "source": [
        "# 5. Linear Systems (Ax = b)\n",
        "\n",
        "Many ML problems boil down to solving a system of linear equations:\n",
        "\n",
        "\\[\n",
        "A x = b\n",
        "\\]\n",
        "\n",
        "- **A** = matrix (data or coefficients)  \n",
        "- **x** = unknown vector (parameters/weights)  \n",
        "- **b** = output vector (observations)\n",
        "\n",
        "Examples in ML:\n",
        "- Linear regression (solving for weights that best fit data).  \n",
        "- Least squares (finding the best approximate solution when the system doesn’t have an exact solution).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpGwZYbVK2F-",
        "outputId": "f0255324-ec0f-474d-a6e4-bf999ec91fbd"
      },
      "outputs": [],
      "source": [
        "# Solve:\n",
        "# 2x + y = 5\n",
        "# x - y = 1\n",
        "\n",
        "A = np.array([[2, 1],\n",
        "              [1, -1]])\n",
        "b = np.array([5, 1])\n",
        "\n",
        "x = np.linalg.solve(A, b)\n",
        "print(\"Solution [x, y]:\", x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wECuEHGIMMjL",
        "outputId": "63339ea0-3629-40a2-96a9-9dfc35e58e89"
      },
      "outputs": [],
      "source": [
        "# Example: more equations than unknowns\n",
        "A = np.array([[1, 1],\n",
        "              [1, 2],\n",
        "              [1, 3]])\n",
        "b = np.array([1, 2, 2.5])\n",
        "\n",
        "# Least squares solution (linear regression style)\n",
        "x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
        "print(\"Best-fit solution [intercept, slope]:\", x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DXczl_qMRww"
      },
      "source": [
        "👉 This is exactly what happens in **linear regression**:  \n",
        "- `A` = design matrix (features)  \n",
        "- `x` = weights (parameters)  \n",
        "- `b` = observed outputs  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4fqfCX5MT1t",
        "outputId": "c24e7fb8-053e-46e0-8975-9dbccccb5ef2"
      },
      "outputs": [],
      "source": [
        "# Dataset: simple 1D input\n",
        "X = np.array([[1, 1],\n",
        "              [1, 2],\n",
        "              [1, 3],\n",
        "              [1, 4]])\n",
        "y = np.array([1, 2, 2.5, 4])\n",
        "\n",
        "# Solve for linear regression coefficients\n",
        "beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
        "print(\"Linear regression coefficients [intercept, slope]:\", beta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can you look up the outputs of `np.linalg.lstsq` and see what else is returned?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UvciLi7MWM1"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Solve the system:\n",
        "   - 3x + y = 9\n",
        "   - 2x - y = 8  \n",
        "\n",
        "2. Construct a dataset with inputs `[1, 2, 3, 4, 5]` and outputs `[2, 3, 4.5, 6, 7.5]`.  \n",
        "   - Build the design matrix `X` with a bias column of ones.  \n",
        "   - Use `np.linalg.lstsq` to fit a linear regression model.  \n",
        "   - What are the slope and intercept?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2DcYRu-P0Yh"
      },
      "source": [
        "Linear systems — slide A, b; see line intersection (solution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii8g7vkLPw6R"
      },
      "outputs": [],
      "source": [
        "@interact(\n",
        "    a11=FloatSlider(min=-5, max=2, step=0.1, value=2, description=\"a₁₁\"),\n",
        "    a12=FloatSlider(min=-5, max=1, step=0.1, value=1, description=\"a₁₂\"),\n",
        "    a21=FloatSlider(min=-5, max=1, step=0.1, value=1, description=\"a₂₁\"),\n",
        "    a22=FloatSlider(min=-5, max=-1, step=0.1, value=-1, description=\"a₂₂\"),\n",
        "    b1=FloatSlider(min=-10, max=4, step=0.1, value=4, description=\"b₁\"),\n",
        "    b2=FloatSlider(min=-10, max=1, step=0.1, value=1, description=\"b₂\"),\n",
        ")\n",
        "def solve_2x2(a11, a12, a21, a22, b1, b2):\n",
        "    A = np.array([[a11, a12], [a21, a22]])\n",
        "    b = np.array([b1, b2])\n",
        "    x = None\n",
        "    try:\n",
        "        x = np.linalg.solve(A, b)\n",
        "    except np.linalg.LinAlgError:\n",
        "        pass\n",
        "\n",
        "    xs = np.linspace(-10, 10, 400)\n",
        "\n",
        "    def line(a, b, c):\n",
        "        return (c - a * xs) / b if b != 0 else np.full_like(xs, np.nan)\n",
        "\n",
        "    y1 = line(a11, a12, b1)\n",
        "    y2 = line(a21, a22, b2)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(xs, y1, label=\"a₁₁x + a₁₂y = b₁\")\n",
        "    plt.plot(xs, y2, label=\"a₂₁x + a₂₂y = b₂\")\n",
        "\n",
        "    if x is not None and np.all(np.isfinite(x)):\n",
        "        plt.plot([x[0]], [x[1]], marker=\"o\", markersize=8, color=\"red\")\n",
        "        ttl = f\"Unique solution: ({x[0]:.2f}, {x[1]:.2f})\"\n",
        "    else:\n",
        "        ttl = \"No unique solution (parallel or dependent lines)\"\n",
        "\n",
        "    plt.axhline(0, alpha=0.3)\n",
        "    plt.axvline(0, alpha=0.3)\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.gca().set_aspect(\"equal\", \"box\")\n",
        "    plt.legend()\n",
        "    plt.title(ttl)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSkJY7pKMsbb"
      },
      "source": [
        "# 6. Determinant & Inverse\n",
        "\n",
        "These concepts tell us whether a matrix is \"invertible\" (non-singular).\n",
        "\n",
        "- **Determinant (det A):**\n",
        "  - If det(A) = 0 → matrix is singular (no unique solution).\n",
        "  - If det(A) ≠ 0 → matrix is invertible.\n",
        "\n",
        "- **Inverse (A⁻¹):**\n",
        "  - Solves `A x = b` as `x = A⁻¹ b`.\n",
        "  - In ML, we rarely compute the inverse directly (unstable, slow).\n",
        "  - Instead, we use numerical solvers (`np.linalg.solve` or `np.linalg.lstsq`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSojNxCgMvu-",
        "outputId": "8ef79a03-2233-4a33-fb3f-cce00ff3948a"
      },
      "outputs": [],
      "source": [
        "# Determinant of a matrix\n",
        "\n",
        "A = np.array([[2, 3],\n",
        "              [1, 4]])\n",
        "\n",
        "detA = np.linalg.det(A)\n",
        "print(\"Determinant of A:\", detA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgyDzwcFMO-x",
        "outputId": "813bfb57-8af0-430d-bcad-842ad7210746"
      },
      "outputs": [],
      "source": [
        "# Compute inverse\n",
        "A_inv = np.linalg.inv(A)\n",
        "print(\"Inverse of A:\\n\", A_inv)\n",
        "\n",
        "# Verify: A @ A_inv ≈ Identity\n",
        "I = A @ A_inv\n",
        "print(\"A @ A_inv:\\n\", I)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1XQ-TcaMzLL"
      },
      "source": [
        "⚠️ In ML:\n",
        "- Don’t compute `A⁻¹` directly for regression (numerical instability).\n",
        "- Use **solvers** instead:\n",
        "  - `np.linalg.solve(A, b)` for exact systems\n",
        "  - `np.linalg.lstsq(A, b)` for least squares\n",
        "\n",
        "👉 But knowing about determinants & inverses helps understand why some problems have no unique solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcgzDrDcM22X"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1. Compute the determinant of:\n",
        "\n",
        "$$\n",
        "B = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "What does it tell you about invertibility?\n",
        "\n",
        "2. Try calling `np.linalg.inv(B)`. What happens?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q88xEIMGNqLc"
      },
      "source": [
        "# 7. Eigenvalues, Eigenvectors & PCA\n",
        "\n",
        "Core idea: some directions **don’t rotate** under a linear transform, they only **scale**.\n",
        "\n",
        "$$\n",
        "A\\mathbf v = \\lambda \\mathbf v\n",
        "$$\n",
        "\n",
        "- $\\mathbf v$ is an **eigenvector** of $A$\n",
        "- $\\lambda$ is the corresponding **eigenvalue**\n",
        "\n",
        "In ML:\n",
        "- **Covariance matrices** have eigenvectors that point along directions of maximum variance.\n",
        "- **PCA** keeps the top-variance directions to compress data with minimal information loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNbrKbUrNsRL",
        "outputId": "e7afd38a-eb42-44ce-8ed0-cc51b2b9fb78"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Toy 2D dataset (rows = samples, cols = features)\n",
        "# Example: [feature_1, feature_2] for 6 samples\n",
        "X = np.array([\n",
        "    [2.0, 1.0],\n",
        "    [2.2, 1.3],\n",
        "    [2.9, 2.7],\n",
        "    [3.1, 2.9],\n",
        "    [3.0, 2.4],\n",
        "    [2.7, 2.2],\n",
        "])\n",
        "\n",
        "# Center the data (zero-mean per feature)\n",
        "Xc = X - X.mean(axis=0)\n",
        "Xc[:3], Xc.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UXKI6fANuDu"
      },
      "source": [
        "For PCA, compute the **covariance matrix** of centered data and take its eigenpairs.\n",
        "\n",
        "- Covariance (for centered $X_c$): $C = \\frac{1}{n-1} X_c^\\top X_c$\n",
        "- Use `np.linalg.eigh` (for symmetric matrices like covariance).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4hNJTQlN3qW",
        "outputId": "b4229aef-d6ea-42d9-e2a6-eee63e0059d5"
      },
      "outputs": [],
      "source": [
        "# Covariance matrix\n",
        "C = np.cov(Xc, rowvar=False)\n",
        "\n",
        "# Eigen-decomposition (symmetric): returns ascending eigenvalues\n",
        "eigvals, eigvecs = np.linalg.eigh(C)\n",
        "\n",
        "# Sort by descending eigenvalue\n",
        "idx = np.argsort(eigvals)[::-1]\n",
        "eigvals = eigvals[idx]\n",
        "eigvecs = eigvecs[:, idx]\n",
        "\n",
        "explained_var_ratio = eigvals / eigvals.sum()\n",
        "\n",
        "eigvals, explained_var_ratio, eigvecs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGnmDNzYN53S"
      },
      "source": [
        "Project data onto principal components:\n",
        "\n",
        "$$\n",
        "Z = X_c W\n",
        "$$\n",
        "\n",
        "where columns of $W$ are the top-$k$ eigenvectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN0AP2lmN9OW",
        "outputId": "dbfeb40c-9d38-4f59-e1b3-b71a19f34d26"
      },
      "outputs": [],
      "source": [
        "# Project onto all PCs (here 2D → 2 PCs)\n",
        "Z = Xc @ eigvecs   # scores in PC space\n",
        "\n",
        "# Keep only the first principal component\n",
        "Z1 = Z[:, [0]]     # (n, 1)\n",
        "# Reconstruct rank-1 approximation in original space\n",
        "X_approx = Z1 @ eigvecs[:, [0]].T\n",
        "\n",
        "explained_var_ratio, Z[:3], X_approx[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "yxdlAq76N_GN",
        "outputId": "b00a25c5-a396-4b08-8df4-e1e4ca9fcb38"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.scatter(Xc[:,0], Xc[:,1], label=\"Centered data\")\n",
        "origin = np.array([[0,0],[0,0]])\n",
        "\n",
        "# draw principal axes (scaled for display)\n",
        "scale = 2*np.sqrt(eigvals)\n",
        "axes = eigvecs * scale\n",
        "\n",
        "plt.plot( [0, axes[0,0]], [0, axes[1,0]], label=\"PC1\", linewidth=3 )\n",
        "plt.plot( [0, axes[0,1]], [0, axes[1,1]], label=\"PC2\", linewidth=3 )\n",
        "\n",
        "plt.axhline(0, alpha=0.3); plt.axvline(0, alpha=0.3)\n",
        "plt.gca().set_aspect('equal', 'box')\n",
        "plt.legend(); plt.title(\"PCA: principal directions on centered data\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zADfsy0fOE71"
      },
      "source": [
        "### Mini Exercise\n",
        "\n",
        "1) Build a 3D dataset `X3` with 10 samples and clear correlation between features\n",
        "   (e.g., make feature 3 ≈ feature 1 + small noise).\n",
        "2) Center the data, compute covariance, eigenvalues/eigenvectors with `np.linalg.eigh`.\n",
        "3) Report the **explained variance ratio**. How many PCs do you need for ≥95% EVR?\n",
        "4) Project the data to the top 2 PCs and reconstruct (rank-2). Compare the reconstruction\n",
        "   error `‖X_c - X_approx‖_F` (Frobenius norm).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-jPeiMKMxg3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgdhfSDMygBDGU3UzVGTWt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aps106",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
